{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8a2e1c5-aa05-466b-be11-b422da4f7c25",
   "metadata": {},
   "source": [
    "# modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b395f2c-139d-4563-9eee-492057582cb8",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e535e21a-6c2f-44d9-aa28-faa91de14ad7",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f1d90b-4a93-4b63-b22d-a20fa107c361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from functools import partial\n",
    "from typing import TypeAlias\n",
    "\n",
    "import hnswlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import spacy\n",
    "from gensim.models import Word2Vec\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69d4d4c-74af-408e-90e7-431639f319d9",
   "metadata": {},
   "source": [
    "### config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650f6dec-d63a-4c18-8806-59186587fa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLE_TEST = True\n",
    "\n",
    "MODELS_WORD2VEC_FOLDER = \"/veld/input/models/\"\n",
    "TEXTS_FOLDER = \"/veld/input/texts/\"\n",
    "PICKLE_FOLDER = \"/veld/storage/cache/\"\n",
    "\n",
    "INDEX_EF_CONSTRUCTION = 100\n",
    "INDEX_M = 16\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8120ca6-8912-4638-a6aa-fe8ff36b5b97",
   "metadata": {},
   "source": [
    "### load_cache_or_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf56699-7b77-4460-96ad-666717e93840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cache_or_run(func):\n",
    "    pickle_file_path = PICKLE_FOLDER + func.__name__ + \".pkl\"\n",
    "    if os.path.exists(pickle_file_path):\n",
    "        with open(pickle_file_path, \"rb\") as f:\n",
    "            result = pickle.load(f)\n",
    "            print(\"load_cache_or_run: loaded from cache at:\", pickle_file_path)\n",
    "    else:\n",
    "        result = func()\n",
    "        with open(pickle_file_path, \"wb\") as f:\n",
    "            pickle.dump(result, f)\n",
    "            print(\"load_cache_or_run: persisted into cache at:\", pickle_file_path)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b4fba7-d100-49d7-9eac-ac7af614d7f3",
   "metadata": {},
   "source": [
    "### load_if_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf054cb-b19e-4f84-8f87-5d70b489f715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_if_test(func):\n",
    "    if ENABLE_TEST:\n",
    "        return load_cache_or_run(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13721081-b7d7-44c9-ab0e-bb2c07f649bd",
   "metadata": {},
   "source": [
    "## data structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc305687-3bc1-4e75-9b65-83220404eaee",
   "metadata": {},
   "source": [
    "### tpye aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e80f266-a429-4f85-9d63-f7d2b62cc73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lemma: TypeAlias = str\n",
    "Word: TypeAlias = str\n",
    "Decade: TypeAlias = int\n",
    "LineNumber: TypeAlias = int\n",
    "WordNumber: TypeAlias = int\n",
    "OccurrenceCount: TypeAlias = int\n",
    "Diff: TypeAlias = float\n",
    "Embedding: TypeAlias = np.ndarray\n",
    "Model: TypeAlias = Word2Vec\n",
    "DecadeList: TypeAlias = list[Decade]\n",
    "\n",
    "# lemma data structures\n",
    "LemmaDiffDict: TypeAlias = dict[Lemma, Diff]\n",
    "LineNumberDict: TypeAlias = dict[LineNumber, list[WordNumber]]\n",
    "LemmaOccurrencePositionDict: TypeAlias = dict[Lemma, LineNumberDict]\n",
    "LemmaOccurrenceCountDict: TypeAlias = dict[Lemma, OccurrenceCount]\n",
    "LemmaWordDict: TypeAlias = dict[Lemma, list[Word]]\n",
    "WordLemmaDict: TypeAlias = dict[Word, Lemma]\n",
    "\n",
    "# index data structure\n",
    "IdToLemmaDict: TypeAlias = dict[int, Lemma]\n",
    "LemmaToIdDict: TypeAlias = dict[Lemma, int]\n",
    "Index: TypeAlias = tuple[hnswlib.Index, LemmaToIdDict, IdToLemmaDict]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58044656-cc17-4f9f-8959-2697f02436ee",
   "metadata": {},
   "source": [
    "### create_decades_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc419e9c-6232-421a-9863-fe087ed40e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decades_list(folder: str, decade_start: int = 155, decade_end: int = 191) -> DecadeList:\n",
    "    decade_list = []\n",
    "    for model_file in os.listdir(folder):\n",
    "        if model_file.endswith(\".bin\"):\n",
    "            decade = int(model_file.split(\".bin\")[0])\n",
    "            decade_list.append(decade)\n",
    "    decade_list = sorted(decade_list)\n",
    "    i_start = 0\n",
    "    i_end = len(decade_list)\n",
    "    for i, decade in enumerate(decade_list):\n",
    "        if decade_start and decade_start == decade:\n",
    "            i_start = i\n",
    "        if decade_end and decade_end == decade:\n",
    "            i_end = i + 1\n",
    "    decade_list = decade_list[i_start:i_end]\n",
    "    print(\"create_decades_list: decade_list:\", decade_list)\n",
    "    return decade_list\n",
    "\n",
    "\n",
    "def create_decades_list_test():\n",
    "    return create_decades_list(MODELS_WORD2VEC_FOLDER)\n",
    "\n",
    "\n",
    "decade_list = load_if_test(create_decades_list_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b39ab93-0e45-43ca-a6dc-e80596674e4f",
   "metadata": {},
   "source": [
    "### load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882f0f7b-69bf-4a35-bca8-8ed00501fb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(decade: Decade) -> Model:\n",
    "    model_path = MODELS_WORD2VEC_FOLDER + str(decade) + \".bin\"\n",
    "    model = Word2Vec.load(model_path)\n",
    "    print(\"load_model_word2vec: model_path:\", model_path)\n",
    "    print(\"load_model_word2vec: model:\", model)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_model_test():\n",
    "    model_174 = load_model(174)\n",
    "    model_175 = load_model(175)\n",
    "    model_176 = load_model(176)\n",
    "    return model_174, model_175, model_176\n",
    "\n",
    "\n",
    "model_174, model_175, model_176 = load_if_test(load_model_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fb75d6-b481-45a9-93d5-51abc7f9b2f0",
   "metadata": {},
   "source": [
    "### create_lemma_word_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355c04cb-9e25-4933-95da-597525cc92ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lemma_word_dicts(model: Model) -> tuple[LemmaWordDict, WordLemmaDict]:\n",
    "    word_lemma_dict = {}\n",
    "    lemma_word_dict = {}\n",
    "    for word in model.wv.index_to_key:\n",
    "        lemma = nlp(word)[0].lemma_\n",
    "        word_list = lemma_word_dict.get(lemma, [])\n",
    "        word_list.append(word)\n",
    "        lemma_word_dict[lemma] = word_list\n",
    "        word_lemma_dict[word] = lemma\n",
    "    print(\"create_lemma_word_dicts: len(lemma_word_dict):\", len(lemma_word_dict))\n",
    "    return lemma_word_dict, word_lemma_dict\n",
    "\n",
    "\n",
    "def create_lemma_word_dicts_test():\n",
    "    lemma_word_dict_174, word_lemma_dict_174 = create_lemma_word_dicts(model_174)\n",
    "    lemma_word_dict_175, word_lemma_dict_175 = create_lemma_word_dicts(model_175)\n",
    "    lemma_word_dict_176, word_lemma_dict_176 = create_lemma_word_dicts(model_176)\n",
    "    return lemma_word_dict_174, word_lemma_dict_174, lemma_word_dict_175, word_lemma_dict_175, lemma_word_dict_176, word_lemma_dict_176\n",
    "\n",
    "\n",
    "lemma_word_dict_174, word_lemma_dict_174, lemma_word_dict_175, word_lemma_dict_175, lemma_word_dict_176, word_lemma_dict_176 = load_if_test(\n",
    "    create_lemma_word_dicts_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0744e237-c5e4-4e89-aa23-97bc1c9fedb8",
   "metadata": {},
   "source": [
    "### create_occurrence_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cce9fe-b5c7-46f9-af1e-249438285932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_occurrence_dicts(decade: Decade, word_lemma_dict: WordLemmaDict) -> tuple[LemmaOccurrencePositionDict, LemmaOccurrenceCountDict]:\n",
    "    lemma_occurrence_position_dict = {}\n",
    "    lemma_occurrence_count_dict = {}\n",
    "    total_occurrence_count = 0\n",
    "    with open(TEXTS_FOLDER + str(decade) + \".txt\", \"r\") as f:\n",
    "        for line_number, line in enumerate(f):\n",
    "            for word_number, word in enumerate(line.rstrip(\"\\n\").split(\" \")):\n",
    "                lemma = word_lemma_dict.get(word)\n",
    "                if lemma:\n",
    "                    line_number_dict: LineNumberDict = lemma_occurrence_position_dict.get(lemma, {})\n",
    "                    word_number_list: list[WordNumber] = line_number_dict.get(line_number, [])\n",
    "                    word_number_list.append(word_number)\n",
    "                    line_number_dict[line_number] = word_number_list\n",
    "                    lemma_occurrence_position_dict[lemma] = line_number_dict\n",
    "                    occurrence_count = lemma_occurrence_count_dict.get(lemma, 0)\n",
    "                    lemma_occurrence_count_dict[lemma] = occurrence_count + 1\n",
    "                    total_occurrence_count += 1\n",
    "    lemma_count = len(lemma_occurrence_count_dict)\n",
    "    occurrence_avg = total_occurrence_count / lemma_count\n",
    "    median_pos = int(lemma_count / 2)\n",
    "    occurrence_median = list(lemma_occurrence_count_dict.values())[median_pos]\n",
    "    print(\"create_occurrence_dicts: lemma_count:\", lemma_count)\n",
    "    print(\"create_occurrence_dicts: total_occurrence_count:\", total_occurrence_count)\n",
    "    print(\"create_occurrence_dicts: occurrence_avg:\", occurrence_avg)\n",
    "    print(\"create_occurrence_dicts: occurrence_median:\", occurrence_median)\n",
    "    return lemma_occurrence_position_dict, lemma_occurrence_count_dict\n",
    "\n",
    "\n",
    "def create_occurrence_dicts_test():\n",
    "    lemma_occurrence_position_dict_174, lemma_occurrence_count_dict_174 = create_occurrence_dicts(174, word_lemma_dict_174)\n",
    "    lemma_occurrence_position_dict_175, lemma_occurrence_count_dict_175 = create_occurrence_dicts(175, word_lemma_dict_175)\n",
    "    lemma_occurrence_position_dict_176, lemma_occurrence_count_dict_176 = create_occurrence_dicts(176, word_lemma_dict_176)\n",
    "    return (\n",
    "        lemma_occurrence_position_dict_174,\n",
    "        lemma_occurrence_count_dict_174,\n",
    "        lemma_occurrence_position_dict_175,\n",
    "        lemma_occurrence_count_dict_175,\n",
    "        lemma_occurrence_position_dict_176,\n",
    "        lemma_occurrence_count_dict_176,\n",
    "    )\n",
    "\n",
    "\n",
    "(\n",
    "    lemma_occurrence_position_dict_174,\n",
    "    lemma_occurrence_count_dict_174,\n",
    "    lemma_occurrence_position_dict_175,\n",
    "    lemma_occurrence_count_dict_175,\n",
    "    lemma_occurrence_position_dict_176,\n",
    "    lemma_occurrence_count_dict_176,\n",
    ") = load_if_test(create_occurrence_dicts_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bec0c8c-8531-49fe-9661-62bd389e96df",
   "metadata": {},
   "source": [
    "### sort_lemma_dict_by_value_to_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fe2804-6089-40f0-a78c-8bba6225eed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_lemma_dict_by_value_to_list(lemma_dict: dict[Lemma, int | float], desc=True) -> list[Lemma]:\n",
    "    if desc:\n",
    "        sort_mod = -1\n",
    "    else:\n",
    "        sort_mod = 1\n",
    "    lemma_list_sorted = [(lemma, value) for lemma, value in lemma_dict.items()]\n",
    "    lemma_list_sorted = [l[0] for l in sorted(lemma_list_sorted, key=lambda x: sort_mod * x[1])]\n",
    "    print(\"sort_lemma_occurrence_count_dict: len(lemma_list_sorted):\", len(lemma_list_sorted))\n",
    "    return lemma_list_sorted\n",
    "\n",
    "\n",
    "def sort_lemma_dict_by_value_to_list_test():\n",
    "    lemma_list_sorted_174 = sort_lemma_dict_by_value_to_list(lemma_occurrence_count_dict_174)\n",
    "    lemma_list_sorted_175 = sort_lemma_dict_by_value_to_list(lemma_occurrence_count_dict_175)\n",
    "    lemma_list_sorted_176 = sort_lemma_dict_by_value_to_list(lemma_occurrence_count_dict_176)\n",
    "    return lemma_list_sorted_174, lemma_list_sorted_175, lemma_list_sorted_176\n",
    "\n",
    "\n",
    "lemma_list_sorted_174, lemma_list_sorted_175, lemma_list_sorted_176 = load_if_test(sort_lemma_dict_by_value_to_list_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6990ae4-568c-4dcf-a24d-436cf14d0015",
   "metadata": {},
   "source": [
    "### sort_lemma_dict_by_value_to_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f66d94-438d-48ef-88a0-b7e97a42a374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_lemma_dict_by_value_to_dict(lemma_dict: dict[Lemma, int | float], desc=True) -> dict:\n",
    "    lemma_list_sorted = sort_lemma_dict_by_value_to_list(lemma_dict, desc)\n",
    "    lemma_dict_new = {}\n",
    "    for lemma in lemma_list_sorted:\n",
    "        lemma_dict_new[lemma] = lemma_dict[lemma]\n",
    "    return lemma_dict_new\n",
    "\n",
    "\n",
    "def sort_lemma_dict_by_value_to_dict_test():\n",
    "    global lemma_occurrence_count_dict_174\n",
    "    global lemma_occurrence_count_dict_175\n",
    "    global lemma_occurrence_count_dict_176\n",
    "    lemma_occurrence_count_dict_174 = sort_lemma_dict_by_value_to_dict(lemma_occurrence_count_dict_174)\n",
    "    lemma_occurrence_count_dict_175 = sort_lemma_dict_by_value_to_dict(lemma_occurrence_count_dict_175)\n",
    "    lemma_occurrence_count_dict_176 = sort_lemma_dict_by_value_to_dict(lemma_occurrence_count_dict_176)\n",
    "    return lemma_occurrence_count_dict_174, lemma_occurrence_count_dict_175, lemma_occurrence_count_dict_176\n",
    "\n",
    "\n",
    "lemma_occurrence_count_dict_174, lemma_occurrence_count_dict_175, lemma_occurrence_count_dict_176 = load_if_test(\n",
    "    sort_lemma_dict_by_value_to_dict_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dc95c7-4a21-4758-9d1c-bbdc946476c4",
   "metadata": {},
   "source": [
    "### sort_by_occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b87c75-7995-4772-8b3d-bb6ae707844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_occurrence(lemma_list_sorted: list[Lemma], sortable_lemma_dict: dict) -> dict:\n",
    "    sortable_lemma_dict_new = {}\n",
    "    for lemma in lemma_list_sorted:\n",
    "        sortable_lemma_dict_new[lemma] = sortable_lemma_dict[lemma]\n",
    "    print(\"sort_by_occurrence: len(sortable_lemma_dict_new):\", len(sortable_lemma_dict_new))\n",
    "    return sortable_lemma_dict_new\n",
    "\n",
    "\n",
    "def sort_by_occurrence_test():\n",
    "    global lemma_occurrence_position_dict_174\n",
    "    global lemma_occurrence_position_dict_175\n",
    "    global lemma_occurrence_position_dict_176\n",
    "    global lemma_word_dict_174\n",
    "    global lemma_word_dict_175\n",
    "    global lemma_word_dict_176\n",
    "    lemma_occurrence_position_dict_174 = sort_by_occurrence(lemma_list_sorted_174, lemma_occurrence_position_dict_174)\n",
    "    lemma_occurrence_position_dict_175 = sort_by_occurrence(lemma_list_sorted_175, lemma_occurrence_position_dict_175)\n",
    "    lemma_occurrence_position_dict_176 = sort_by_occurrence(lemma_list_sorted_176, lemma_occurrence_position_dict_176)\n",
    "    lemma_word_dict_174 = sort_by_occurrence(lemma_list_sorted_174, lemma_word_dict_174)\n",
    "    lemma_word_dict_175 = sort_by_occurrence(lemma_list_sorted_175, lemma_word_dict_175)\n",
    "    lemma_word_dict_176 = sort_by_occurrence(lemma_list_sorted_176, lemma_word_dict_176)\n",
    "    return (\n",
    "        lemma_occurrence_position_dict_174,\n",
    "        lemma_occurrence_position_dict_175,\n",
    "        lemma_occurrence_position_dict_176,\n",
    "        lemma_word_dict_174,\n",
    "        lemma_word_dict_175,\n",
    "        lemma_word_dict_176,\n",
    "    )\n",
    "\n",
    "\n",
    "(\n",
    "    lemma_occurrence_position_dict_174,\n",
    "    lemma_occurrence_position_dict_175,\n",
    "    lemma_occurrence_position_dict_176,\n",
    "    lemma_word_dict_174,\n",
    "    lemma_word_dict_175,\n",
    "    lemma_word_dict_176,\n",
    ") = load_if_test(sort_by_occurrence_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f7a697-a6ec-4259-a377-10d6dbf05914",
   "metadata": {},
   "source": [
    "### get_occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10ae976-660d-4166-9300-9edf60e010ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_occurrences(\n",
    "    decade: Decade,\n",
    "    line_number_dict: LineNumberDict,\n",
    "    max_elem: int = None,\n",
    "    highlight_lemma: bool = True,\n",
    "    keep_lemma: bool = True,\n",
    ") -> list[str]:\n",
    "    text_list = []\n",
    "    with open(TEXTS_FOLDER + str(decade) + \".txt\", \"r\") as f:\n",
    "        num_print = 0\n",
    "        for line_number, line in enumerate(f):\n",
    "            word_number_list = line_number_dict.get(line_number)\n",
    "            if word_number_list:\n",
    "                word_number_set = set(word_number_list)\n",
    "                text = \"\"\n",
    "                for word_number, word in enumerate(line.rstrip(\"\\n\").split(\" \")):\n",
    "                    if word_number in word_number_set and highlight_lemma and keep_lemma:\n",
    "                        text += \" ### \" + word + \" ###\"\n",
    "                    elif word_number not in word_number_set or (not highlight_lemma and keep_lemma):\n",
    "                        text += \" \" + word\n",
    "                    else:\n",
    "                        pass\n",
    "                text_list.append(text)\n",
    "                num_print += 1\n",
    "                if max_elem and num_print == max_elem:\n",
    "                    break\n",
    "    return text_list\n",
    "\n",
    "\n",
    "if ENABLE_TEST:\n",
    "    print(get_occurrences(175, lemma_occurrence_position_dict_175[\"gehen\"], max_elem=1))\n",
    "    print(get_occurrences(175, lemma_occurrence_position_dict_175[\"gehen\"], max_elem=1, highlight_lemma=False))\n",
    "    print(get_occurrences(175, lemma_occurrence_position_dict_175[\"gehen\"], max_elem=1, keep_lemma=False))\n",
    "    print(len(get_occurrences(175, lemma_occurrence_position_dict_175[\"gehen\"], max_elem=None, highlight_lemma=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf229bb-96cd-432a-9b93-f5558adf263e",
   "metadata": {},
   "source": [
    "### create_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581a8a67-0e81-4437-b884-73477a6530bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(lemma_word_dict: LemmaWordDict, model: Model) -> Index:\n",
    "    id_to_lemma_dict: IdToLemmaDict = {}\n",
    "    lemma_to_id_dict: LemmaToIdDict = {}\n",
    "    embedding_array = []\n",
    "    for lemma_id, (lemma, word_list) in enumerate(lemma_word_dict.items()):\n",
    "        id_to_lemma_dict[lemma_id] = lemma\n",
    "        lemma_to_id_dict[lemma] = lemma_id\n",
    "        word_embedding_list = [model.wv[word] for word in word_list]\n",
    "        embedding_average = np.mean(np.array(word_embedding_list), axis=0)\n",
    "        embedding_normalized = embedding_average / np.linalg.norm(embedding_average)\n",
    "        embedding_array.append(embedding_normalized)\n",
    "    embedding_array = np.array(embedding_array)\n",
    "    max_elements = len(embedding_array)\n",
    "    dim = embedding_array[0].shape[0]\n",
    "    hnsw_index = hnswlib.Index(space=\"cosine\", dim=dim)\n",
    "    hnsw_index.init_index(max_elements=max_elements, ef_construction=INDEX_EF_CONSTRUCTION, M=INDEX_M)\n",
    "    hnsw_index.add_items(embedding_array, ids=list(id_to_lemma_dict.keys()))\n",
    "    index = (hnsw_index, lemma_to_id_dict, id_to_lemma_dict)\n",
    "    print(\"create_lemma_dict_and_index: hnsw_index.get_current_count:\", hnsw_index.get_current_count())\n",
    "    return index\n",
    "\n",
    "\n",
    "def create_index_test():\n",
    "    index_174 = create_index(lemma_word_dict_174, model_174)\n",
    "    index_175 = create_index(lemma_word_dict_175, model_175)\n",
    "    index_176 = create_index(lemma_word_dict_176, model_176)\n",
    "    return index_174, index_175, index_176\n",
    "\n",
    "\n",
    "index_174, index_175, index_176 = load_if_test(create_index_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe4294a-1b34-404e-bfc5-6a2ac3c3971f",
   "metadata": {},
   "source": [
    "## vector and index functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bef6a1-10d1-425e-9dd1-9ff8acc78734",
   "metadata": {},
   "source": [
    "### get_common_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f5a1dd-70ac-4a5e-848a-d3dc3d496d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_lemma(*lemma_dict_list):\n",
    "    lemma_set_list = []\n",
    "    for lemma_dict in lemma_dict_list:\n",
    "        lemma_set_list.append(set(lemma_dict.keys()))\n",
    "    return set.intersection(*lemma_set_list)\n",
    "\n",
    "\n",
    "if ENABLE_TEST:\n",
    "    print(get_common_lemma({\"a\": 1, \"b\": 2}, {\"a\": 1, \"c\": 3}, {\"a\": 1, \"b\": 2, \"c\": 3}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1797b6e1-33f3-45e0-a711-86d51d547d7c",
   "metadata": {},
   "source": [
    "### calculate_cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f02bef-6c1c-47b2-ba0e-ca03eea28722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cos_sim(embedding_a: np.ndarray, embedding_b: np.ndarray) -> float:\n",
    "    return np.dot(embedding_a, embedding_b) / (np.linalg.norm(embedding_a) * np.linalg.norm(embedding_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d60524-a615-4213-bc2a-d05edfa78180",
   "metadata": {},
   "source": [
    "### calculate_cos_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611c26e6-cf9a-4fed-bb75-be68aaa67d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cos_distance(embedding_a: np.ndarray, embedding_b: np.ndarray) -> float:\n",
    "    return 1 - calculate_cos_sim(embedding_a, embedding_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130c89ec-42ce-4083-bef9-526ac4e0b1fb",
   "metadata": {},
   "source": [
    "### query_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2895f1fa-41ed-4b9c-855c-722c1c79bcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_embedding(index: Index, lemma: Lemma) -> float:\n",
    "    embedding = index[0].get_items([index[1][lemma]])[0]\n",
    "    return embedding\n",
    "\n",
    "\n",
    "if ENABLE_TEST:\n",
    "    embedding = query_embedding(index_175, \"gehen\")\n",
    "    print(embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deed2196-185b-45eb-9f3e-e356a4027ab4",
   "metadata": {},
   "source": [
    "### query_related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77994581-272b-48e6-9c75-7a0ff66871e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_related(\n",
    "    index: Index,\n",
    "    lemma: Lemma,\n",
    "    n: int = 10,\n",
    "    return_as_dict: bool = True,\n",
    "    keep_search: bool = False,\n",
    ") -> dict[str, float] | list[str]:\n",
    "    result = None\n",
    "    try:\n",
    "        id_embedding = index[1][lemma]\n",
    "    except:\n",
    "        print(\"not found\")\n",
    "    else:\n",
    "        if keep_search:\n",
    "            distances_start = 0\n",
    "        else:\n",
    "            n += 1\n",
    "            distances_start = 1\n",
    "        embedding = index[0].get_items([id_embedding])[0]\n",
    "        ids, distances = index[0].knn_query(embedding.reshape(1, -1), k=n)\n",
    "        if return_as_dict:\n",
    "            result = {}\n",
    "        else:\n",
    "            result = []\n",
    "        for id_other, distance in list(zip(ids[0], distances[0]))[distances_start:]:\n",
    "            lemma_related = index[2][id_other]\n",
    "            if return_as_dict:\n",
    "                result[lemma_related] = distance\n",
    "            else:\n",
    "                result.append(lemma_related)\n",
    "    return result\n",
    "\n",
    "\n",
    "if ENABLE_TEST:\n",
    "    print(query_related(index_175, \"gehen\", n=10))\n",
    "    print(query_related(index_176, \"gehen\", n=10))\n",
    "    print(query_related(index_176, \"gehen\", n=10, return_as_dict=False, keep_search=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f9c5f7-d9ca-45c1-b01a-c062c6362f6c",
   "metadata": {},
   "source": [
    "### calculate_average_sentence_embedding_from_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc1293c-30d5-4155-9ae4-91a0bd8c99f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_sentence_embedding_from_sentence(\n",
    "    sentence: str,\n",
    "    index: Index,\n",
    "    word_lemma_dict: WordLemmaDict,\n",
    "    show_exception: bool = False,\n",
    ") -> np.ndarray:\n",
    "    embedding_list = []\n",
    "    for word in sentence.split(\" \"):\n",
    "        try:\n",
    "            embedding_list.append(query_embedding(index, word_lemma_dict[word]))\n",
    "        except Exception as ex:\n",
    "            if show_exception:\n",
    "                print(ex, word)\n",
    "    embedding_avg = np.mean(np.array(embedding_list), axis=0)\n",
    "    return embedding_avg\n",
    "\n",
    "\n",
    "if ENABLE_TEST:\n",
    "    v1 = calculate_average_sentence_embedding_from_sentence(\n",
    "        \"der mensch ist in dem haus\", index_175, word_lemma_dict_174, show_exception=True\n",
    "    )\n",
    "    v2 = calculate_average_sentence_embedding_from_sentence(\n",
    "        \"der mann ist in der hütte\", index_175, word_lemma_dict_175, show_exception=True\n",
    "    )\n",
    "    v3 = calculate_average_sentence_embedding_from_sentence(\n",
    "        \"die ziege ist auf dem feld\", index_175, word_lemma_dict_175, show_exception=True\n",
    "    )\n",
    "    print(calculate_cos_sim(v1, v2))\n",
    "    print(calculate_cos_sim(v2, v3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161e3151-5932-4f04-ae8a-e39e33eefc55",
   "metadata": {},
   "source": [
    "### calculate_average_sentence_embedding_from_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd2fa6e-5174-4ef8-b72f-5a39696e2e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_sentence_embedding_from_lemma(\n",
    "    decade: Decade,\n",
    "    line_number_dict: LineNumberDict,\n",
    "    index: Index,\n",
    "    word_lemma_dict: WordLemmaDict,\n",
    "    max_sentences: int = 2,\n",
    ") -> np.ndarray:\n",
    "    sentence_list = get_occurrences(decade, line_number_dict, max_elem=max_sentences, highlight_lemma=False)\n",
    "    sentence_embedding_dict = {}\n",
    "    for sentence in sentence_list:\n",
    "        sentence_embedding_dict[sentence] = calculate_average_sentence_embedding_from_sentence(sentence, index, word_lemma_dict)\n",
    "    return sentence_embedding_dict\n",
    "\n",
    "\n",
    "if ENABLE_TEST:\n",
    "    sentence_embedding_dict = calculate_average_sentence_embedding_from_lemma(\n",
    "        175,\n",
    "        lemma_occurrence_position_dict_175[\"gehen\"],\n",
    "        index_175,\n",
    "        word_lemma_dict_175,\n",
    "        max_sentences=1,\n",
    "    )\n",
    "    for sentence, embedding in sentence_embedding_dict.items():\n",
    "        print(sentence)\n",
    "        print(embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f908ad80-d33c-4bde-a049-e42f89136c02",
   "metadata": {},
   "source": [
    "### create_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a54291d-0139-468a-9668-6aad14749cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tsne(embeddings, perplexity=None):\n",
    "    if perplexity is None:\n",
    "        perplexity = 5\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "    embeddings_reduced = tsne.fit_transform(np.array(embeddings))\n",
    "    return embeddings_reduced\n",
    "\n",
    "\n",
    "if ENABLE_TEST:\n",
    "    embeddings_reduced = create_tsne([query_embedding(index_175, \"gehen\"), query_embedding(index_175, \"laufen\")], perplexity=1)\n",
    "    print(embeddings_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9e6a87-dcb9-4f90-831d-192ee2366d19",
   "metadata": {},
   "source": [
    "## difference analysis functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ab5ed1-54c5-44f2-aeff-f24ca178d222",
   "metadata": {},
   "source": [
    "### create_diff_of_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73940986-09cc-44e9-8790-b990f1341e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_diff_of_lemma(\n",
    "    index_a: Index,\n",
    "    index_b: Index,\n",
    "    lemma_occurrence_count_dict_a: LemmaOccurrenceCountDict,\n",
    "    lemma_occurrence_count_dict_b: LemmaOccurrenceCountDict,\n",
    "    lemma: Lemma,\n",
    ") -> float:\n",
    "    diff = None\n",
    "\n",
    "    # key and dict synchronization\n",
    "    distances_a_dict = query_related(index_a, lemma, n=100)\n",
    "    distances_b_dict = query_related(index_b, lemma, n=100)\n",
    "    embedding_index_a_lemma = query_embedding(index_a, lemma)\n",
    "    embedding_index_b_lemma = query_embedding(index_b, lemma)\n",
    "    if distances_a_dict and distances_b_dict:\n",
    "        distances_a_lemma_set = set(distances_a_dict.keys())\n",
    "        distances_b_lemma_set = set(distances_b_dict.keys())\n",
    "        lemma_all = set()\n",
    "        for lemma_a in distances_a_lemma_set:\n",
    "            is_in_both = True\n",
    "            if lemma_a not in distances_b_lemma_set:\n",
    "                try:\n",
    "                    embedding_index_b_lemma_a = query_embedding(index_b, lemma_a)\n",
    "                except:\n",
    "                    is_in_both = False\n",
    "                else:\n",
    "                    distances_b_dict[lemma_a] = calculate_cos_distance(embedding_index_a_lemma, embedding_index_b_lemma_a)\n",
    "            if is_in_both:\n",
    "                lemma_all.add(lemma_a)\n",
    "        for lemma_b in distances_b_lemma_set:\n",
    "            is_in_both = True\n",
    "            if lemma_b not in distances_a_lemma_set:\n",
    "                try:\n",
    "                    embedding_index_a_lemma_b = query_embedding(index_a, lemma_b)\n",
    "                except:\n",
    "                    is_in_both = False\n",
    "                else:\n",
    "                    distances_a_dict[lemma_b] = calculate_cos_distance(embedding_index_b_lemma, embedding_index_a_lemma_b)\n",
    "            if is_in_both:\n",
    "                lemma_all.add(lemma_b)\n",
    "\n",
    "        # difference calculation\n",
    "        diff = 0\n",
    "        for lemma_related in lemma_all:\n",
    "            distance_a = distances_a_dict[lemma_related]\n",
    "            distance_b = distances_b_dict[lemma_related]\n",
    "            occurrence_count_avg = (lemma_occurrence_count_dict_a[lemma_related] + lemma_occurrence_count_dict_b[lemma_related]) / 2\n",
    "            diff += (abs(distance_a - distance_b) / np.sqrt(occurrence_count_avg)) * occurrence_count_avg\n",
    "        diff /= len(lemma_all)\n",
    "\n",
    "    return diff\n",
    "\n",
    "\n",
    "if ENABLE_TEST:\n",
    "    print(create_diff_of_lemma(index_174, index_175, lemma_occurrence_count_dict_174, lemma_occurrence_count_dict_175, \"gehen\"))\n",
    "    print(create_diff_of_lemma(index_175, index_176, lemma_occurrence_count_dict_175, lemma_occurrence_count_dict_176, \"brauen\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2902c5-a731-451d-887b-dd07f55929d3",
   "metadata": {},
   "source": [
    "### create_diff_dict_from_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70615ba-1a79-407e-a9b6-52546bfddb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_diff_dict_from_index(\n",
    "    index_a: Index,\n",
    "    index_b: Index,\n",
    "    lemma_occurrence_count_dict_a: LemmaOccurrenceCountDict,\n",
    "    lemma_occurrence_count_dict_b: LemmaOccurrenceCountDict,\n",
    "    min_occurrence: int = None,\n",
    "    max_occurrence: int = None,\n",
    ") -> LemmaDiffDict:\n",
    "    diff_list = []\n",
    "    lemma_common = get_common_lemma(index_a[1], index_b[1])\n",
    "    for lemma in lemma_common:\n",
    "        count_a = lemma_occurrence_count_dict_a[lemma]\n",
    "        count_b = lemma_occurrence_count_dict_b[lemma]\n",
    "        if (min_occurrence is None or (count_a >= min_occurrence and count_b >= min_occurrence)) and (\n",
    "            max_occurrence is None or (count_a <= max_occurrence and count_b <= max_occurrence)\n",
    "        ):\n",
    "            diff = create_diff_of_lemma(index_a, index_b, lemma_occurrence_count_dict_a, lemma_occurrence_count_dict_b, lemma)\n",
    "            diff_list.append((lemma, diff))\n",
    "    diff_list = sorted(diff_list, key=lambda x: -x[1])\n",
    "    lemma_diff_dict = {lemma: diff for lemma, diff in diff_list}\n",
    "    print(\"create_diff_dict_from_index: len(lemma_diff_dict):\", len(lemma_diff_dict))\n",
    "    return lemma_diff_dict\n",
    "\n",
    "\n",
    "def create_diff_dict_from_index_test():\n",
    "    lemma_diff_174_175_dict = create_diff_dict_from_index(\n",
    "        index_174,\n",
    "        index_175,\n",
    "        lemma_occurrence_count_dict_174,\n",
    "        lemma_occurrence_count_dict_175,\n",
    "        min_occurrence=100,\n",
    "        max_occurrence=1000,\n",
    "    )\n",
    "    lemma_diff_175_176_dict = create_diff_dict_from_index(\n",
    "        index_175,\n",
    "        index_176,\n",
    "        lemma_occurrence_count_dict_175,\n",
    "        lemma_occurrence_count_dict_176,\n",
    "        min_occurrence=100,\n",
    "        max_occurrence=1000,\n",
    "    )\n",
    "    return lemma_diff_174_175_dict, lemma_diff_175_176_dict\n",
    "\n",
    "\n",
    "lemma_diff_174_175_dict, lemma_diff_175_176_dict = load_if_test(create_diff_dict_from_index_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a8bc7c-b178-4723-a0d9-090faa075350",
   "metadata": {},
   "source": [
    "### create_procrustes_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b30da27-a2cf-4826-904c-d80c26ec68ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_procrustes_alignment(\n",
    "    index_a: Index,\n",
    "    index_b: Index,\n",
    "    lemma_occurrence_count_dict_a: LemmaOccurrenceCountDict,\n",
    "    lemma_occurrence_count_dict_b: LemmaOccurrenceCountDict,\n",
    "):\n",
    "\n",
    "    # create overlap matrices with embeddings weighted by count of occurrence\n",
    "    common_lemma = get_common_lemma(lemma_occurrence_count_dict_a, lemma_occurrence_count_dict_b)\n",
    "    overlap_matrix_a = []\n",
    "    overlap_matrix_b = []\n",
    "    print(\"create_procrustes_alignment: len(common_lemma):\", len(common_lemma))\n",
    "    for lemma in common_lemma:\n",
    "        occurrence_count_sqrt = np.sqrt((lemma_occurrence_count_dict_a[lemma] + lemma_occurrence_count_dict_b[lemma]) / 2)\n",
    "        embedding_a = query_embedding(index_a, lemma) * occurrence_count_sqrt\n",
    "        embedding_b = query_embedding(index_b, lemma) * occurrence_count_sqrt\n",
    "        overlap_matrix_a.append(embedding_a)\n",
    "        overlap_matrix_b.append(embedding_b)\n",
    "    overlap_matrix_a = np.stack(overlap_matrix_a)\n",
    "    overlap_matrix_b = np.stack(overlap_matrix_b)\n",
    "\n",
    "    # do procrustes transformation\n",
    "    r, _ = orthogonal_procrustes(overlap_matrix_b, overlap_matrix_a)\n",
    "    matrix_b = []\n",
    "    index_b_hnsw, lemma_to_id_b, id_to_lemma_b = index_b\n",
    "    index_b_id_to_lemma_keys = list(id_to_lemma_b.keys())\n",
    "    for i in index_b_id_to_lemma_keys:\n",
    "        embedding_b = index_b_hnsw.get_items([i])[0]\n",
    "        matrix_b.append(embedding_b)\n",
    "    matrix_b = np.stack(matrix_b)\n",
    "    matrix_b_aligned = matrix_b @ r\n",
    "    matrix_b_aligned_normalized = matrix_b_aligned / np.linalg.norm(matrix_b_aligned, axis=1, keepdims=True)\n",
    "    print(\"create_procrustes_alignment: matrix_b_aligned.shape:\", matrix_b_aligned.shape)\n",
    "\n",
    "    # create new index data structure\n",
    "    index_b_aligned = hnswlib.Index(space=\"cosine\", dim=index_b[0].dim)\n",
    "    index_b_aligned.init_index(max_elements=index_b[0].get_max_elements(), ef_construction=INDEX_EF_CONSTRUCTION, M=INDEX_M)\n",
    "    index_b_aligned.add_items(matrix_b_aligned_normalized, index_b_id_to_lemma_keys)\n",
    "\n",
    "    return (index_b_aligned, index_b[1], index_b[2])\n",
    "\n",
    "\n",
    "def create_procrustes_alignment_test():\n",
    "    index_aligned_175 = create_procrustes_alignment(\n",
    "        index_174,\n",
    "        index_175,\n",
    "        lemma_occurrence_count_dict_174,\n",
    "        lemma_occurrence_count_dict_175,\n",
    "    )\n",
    "    index_aligned_176 = create_procrustes_alignment(\n",
    "        index_aligned_175,\n",
    "        index_176,\n",
    "        lemma_occurrence_count_dict_175,\n",
    "        lemma_occurrence_count_dict_176,\n",
    "    )\n",
    "    return index_aligned_175, index_aligned_176\n",
    "\n",
    "\n",
    "index_aligned_175, index_aligned_176 = load_if_test(create_procrustes_alignment_test)\n",
    "if ENABLE_TEST:\n",
    "    for lemma in [\"gehen\", \"und\", \"wohnen\", \"brauen\", \"Fürst\"]:\n",
    "        print(\"create_procrustes_alignment: lemma:\", lemma)\n",
    "        embedding_174 = query_embedding(index_174, lemma)\n",
    "        embedding_175 = query_embedding(index_175, lemma)\n",
    "        embedding_176 = query_embedding(index_176, lemma)\n",
    "        embedding_aligned_175 = query_embedding(index_aligned_175, lemma)\n",
    "        embedding_aligned_176 = query_embedding(index_aligned_176, lemma)\n",
    "        cos_sim_174_175 = calculate_cos_sim(embedding_174, embedding_175)\n",
    "        cos_sim_175_176 = calculate_cos_sim(embedding_175, embedding_176)\n",
    "        cos_sim_aligned_174_175 = calculate_cos_sim(embedding_174, embedding_aligned_175)\n",
    "        cos_sim_aligned_175_176 = calculate_cos_sim(embedding_aligned_175, embedding_aligned_176)\n",
    "        print(\"create_procrustes_alignment:\", \"cos_sim_174_175:\", cos_sim_174_175)\n",
    "        print(\"create_procrustes_alignment:\", \"cos_sim_175_176:\", cos_sim_175_176)\n",
    "        print(\"create_procrustes_alignment:\", \"cos_sim_aligned_174_175:\", cos_sim_aligned_174_175)\n",
    "        print(\"create_procrustes_alignment:\", \"cos_sim_aligned_175_176:\", cos_sim_aligned_175_176)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c26b65a-15b9-4b23-9c80-32052b9a829d",
   "metadata": {},
   "source": [
    "### calculate_trajectory_from_diff_per_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ca5fc5-3503-4750-815e-45641b675a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_trajectory_from_diff_per_lemma(index_a: Index, index_b: Index, index_c: Index, lemma: Lemma):\n",
    "    a = query_embedding(index_a, lemma)\n",
    "    b = query_embedding(index_b, lemma)\n",
    "    c = query_embedding(index_c, lemma)\n",
    "    ab = a - b\n",
    "    bc = b - c\n",
    "    ab_bc_trajectory = np.dot(ab, bc)\n",
    "    return ab_bc_trajectory\n",
    "\n",
    "\n",
    "if ENABLE_TEST:\n",
    "    for lemma in [\"gehen\", \"und\", \"wohnen\", \"brauen\", \"Fürst\"]:\n",
    "        ab_bc_trajectory = calculate_trajectory_from_diff_per_lemma(index_174, index_aligned_175, index_aligned_176, lemma)\n",
    "        print(\"calculate_trajectory_from_diff_per_lemma:\", lemma, ab_bc_trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81efd97f-f201-47e1-920b-df23a56fbd72",
   "metadata": {},
   "source": [
    "### calculate_trajectory_from_diff_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38130a60-c9d9-415a-9b0d-5d5163f2bf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_trajectory_from_diff_all(index_a: Index, index_b: Index, index_c: Index):\n",
    "    lemma_trajectory_diff_list = []\n",
    "    common_lemma = get_common_lemma(index_a[1], index_b[1], index_c[1])\n",
    "    for lemma in common_lemma:\n",
    "        lemma_trajectory_diff = calculate_trajectory_from_diff_per_lemma(index_a, index_b, index_c, lemma)\n",
    "        lemma_trajectory_diff_list.append((lemma, lemma_trajectory_diff))\n",
    "    lemma_trajectory_diff_list = sorted(lemma_trajectory_diff_list, key=lambda x: -x[1])\n",
    "    lemma_trajectory_diff_dict = {l: d for l, d in lemma_trajectory_diff_list}\n",
    "    return lemma_trajectory_diff_dict\n",
    "\n",
    "\n",
    "if ENABLE_TEST:\n",
    "    lemma_trajectory_diff_dict_174_175_176 = calculate_trajectory_from_diff_all(index_174, index_aligned_175, index_aligned_176)\n",
    "    print(\"len(lemma_trajectory_diff_dict_174_175_176)\", len(lemma_trajectory_diff_dict_174_175_176))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9749408-008f-4a06-bb7f-f579006fdaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_lemma_value_dict(lemma_value_dict):\n",
    "    move = min(lemma_value_dict.values())\n",
    "    lemma_value_dict_normalized = {\n",
    "        lemma: diff - move\n",
    "        for lemma, diff in lemma_value_dict.items()\n",
    "    }\n",
    "    scale = 2 / max(lemma_value_dict_normalized.values())\n",
    "    lemma_value_dict_normalized = {\n",
    "        lemma: diff * scale\n",
    "        for lemma, diff in lemma_value_dict_normalized.items()\n",
    "    }\n",
    "    lemma_value_dict_normalized = {\n",
    "        lemma: diff - 1\n",
    "        for lemma, diff in lemma_value_dict_normalized.items()\n",
    "    }\n",
    "    print(\"normalize_lemma_value_dict: len(lemma_value_dict_normalized):\", len(lemma_value_dict_normalized))\n",
    "    return lemma_value_dict_normalized\n",
    "\n",
    "\n",
    "if ENABLE_TEST:\n",
    "    lemma_diff_175_176_dict_normalized = normalize_lemma_value_dict(lemma_diff_175_176_dict)\n",
    "    lemma_trajectory_diff_dict_174_175_176_normalized = normalize_lemma_value_dict(lemma_trajectory_diff_dict_174_175_176)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5acd79d-0484-47d3-984c-718bc0b8e307",
   "metadata": {},
   "source": [
    "## plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e523ce57-2502-4fcc-bd36-6bbd87d369a3",
   "metadata": {},
   "source": [
    "### plot_tsne_from_labels_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd7e74a-c798-48c8-850c-e61abea2fa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne_from_labels_embeddings(\n",
    "    labels: list[str],\n",
    "    embeddings: list[np.ndarray],\n",
    "    title: str = None,\n",
    "    height: int = None,\n",
    "    width: int = None,\n",
    "    rotation_degree: int = None,\n",
    "    perplexity: int = None,\n",
    "):\n",
    "    reduced_vectors_tsne = create_tsne(embeddings, perplexity)\n",
    "\n",
    "    if rotation_degree:\n",
    "        angle_rad = np.deg2rad(-rotation_degree)\n",
    "        rotation_matrix = np.array([[np.cos(angle_rad), -np.sin(angle_rad)], [np.sin(angle_rad), np.cos(angle_rad)]])\n",
    "        reduced_vectors_tsne = reduced_vectors_tsne @ rotation_matrix.T\n",
    "\n",
    "    if height is None:\n",
    "        height = 800\n",
    "    if width is None:\n",
    "        width = 800\n",
    "    fig = px.scatter(\n",
    "        x=reduced_vectors_tsne[:, 0],\n",
    "        y=reduced_vectors_tsne[:, 1],\n",
    "        text=labels,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        title=title,\n",
    "    )\n",
    "    fig.update_layout(xaxis=dict(title=None, showticklabels=False), yaxis=dict(title=None, showticklabels=False))\n",
    "    fig.update_traces(\n",
    "        marker=dict(size=10),\n",
    "        textposition=\"bottom center\",\n",
    "        textfont=dict(size=12),\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "if ENABLE_TEST:\n",
    "    lemma_list = query_related(index_175, \"gehen\", keep_search=True, n=20, return_as_dict=False)\n",
    "    embedding_list = []\n",
    "    for lemma in lemma_list:\n",
    "        embedding_list.append(query_embedding(index_175, lemma))\n",
    "    plot_tsne_from_labels_embeddings(lemma_list, embedding_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ef5750-3378-4d81-a89a-4c0b38c16339",
   "metadata": {},
   "source": [
    "### plot_tsne_from_lemma_and_related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883ab989-c2ac-499d-99f1-de8a7302c034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne_from_lemma_and_related(\n",
    "    index: Index,\n",
    "    lemma: str = None,\n",
    "    n: int = 100,\n",
    "    title: str = None,\n",
    "    height: int = None,\n",
    "    width: int = None,\n",
    "    rotation_degree: int = None,\n",
    "):\n",
    "    lemma_list = query_related(index, lemma, n=n, keep_search=True, return_as_dict=False)\n",
    "    embedding_list = []\n",
    "    for lemma in lemma_list:\n",
    "        embedding_list.append(query_embedding(index_175, lemma))\n",
    "    plot_tsne_from_labels_embeddings(lemma_list, embedding_list)\n",
    "\n",
    "\n",
    "if ENABLE_TEST:\n",
    "    plot_tsne_from_lemma_and_related(index_175, lemma=\"gehen\", n=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef5f43a-bf3e-428d-88bd-09ec9b0b7926",
   "metadata": {},
   "source": [
    "### plot_lemma_with_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94128bff-5d4e-471e-b65f-37ad50dba3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lemma_with_value(lemma_value_dict: LemmaDiffDict, title: str = None):\n",
    "    lemma_list = []\n",
    "    diff_list = []\n",
    "    for lemma, diff in lemma_value_dict.items():\n",
    "        lemma_list.append(lemma)\n",
    "        diff_list.append(diff)\n",
    "    fig = px.scatter(x=lemma_list, y=diff_list, title=title)\n",
    "    fig.update_layout(xaxis_title=None, yaxis_title=None)\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "if ENABLE_TEST:\n",
    "    plot_lemma_with_value(lemma_occurrence_count_dict_174, title=\"lemma occurrence count\")\n",
    "    plot_lemma_with_value(lemma_diff_174_175_dict, title=\"lemma relative diffs\")\n",
    "    plot_lemma_with_value(lemma_trajectory_diff_dict_174_175_176, title=\"lemma trajectory of diffs 174-175-176\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fe0722-f900-4c39-ae9e-66ef7bdc11d3",
   "metadata": {},
   "source": [
    "### plot_trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c0cf9f-79c9-43a8-a3cd-b1218e3aabda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectory(lemma_decade_embdding_dict: dict[Lemma, dict[Decade, Embedding]], perplexity=None):\n",
    "\n",
    "    # prepare data\n",
    "    global_labels_list = []\n",
    "    global_embeddings_list = []\n",
    "    group_end_position_list = []\n",
    "    position_count = 0\n",
    "    for lemma, decade_embedding_dict in lemma_decade_embdding_dict.items():\n",
    "        for decade, embedding in decade_embedding_dict.items():\n",
    "            global_labels_list.append(str(decade) + \":\" + lemma)\n",
    "            global_embeddings_list.append(embedding)\n",
    "            position_count += 1\n",
    "        group_end_position_list.append(position_count)\n",
    "    if 1 < len(global_embeddings_list) < 6:\n",
    "        perplexity = len(global_embeddings_list) - 1\n",
    "    else:\n",
    "        perplexity = None\n",
    "    lemma_embeddings_reduced_array = create_tsne(global_embeddings_list, perplexity=perplexity)\n",
    "\n",
    "    # create plot\n",
    "    fig = go.Figure()\n",
    "    group_start_position = 0\n",
    "    for group_end_position in group_end_position_list:\n",
    "        lemma_respective_embeddings = lemma_embeddings_reduced_array[group_start_position:group_end_position]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=lemma_respective_embeddings[:, 0],\n",
    "                y=lemma_respective_embeddings[:, 1],\n",
    "                mode=\"lines\",\n",
    "            )\n",
    "        )\n",
    "        group_start_position = group_end_position\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=lemma_embeddings_reduced_array[:, 0],\n",
    "            y=lemma_embeddings_reduced_array[:, 1],\n",
    "            mode=\"markers+text\",\n",
    "            text=global_labels_list,\n",
    "            textposition=\"top center\",\n",
    "        )\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        showlegend=False,\n",
    "        width=800,\n",
    "        height=800,\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "if ENABLE_TEST:\n",
    "    lemma_list = [\"der\", \"gefunden\", \"Nachmittag\"]\n",
    "    lemma_decade_embdding_dict = {}\n",
    "    print(\"trajectories:\")\n",
    "    for lemma in lemma_list:\n",
    "        print(lemma, lemma_trajectory_diff_dict_174_175_176[lemma])\n",
    "        lemma_decade_embdding_dict[lemma] = {\n",
    "            174: query_embedding(index_174, lemma),\n",
    "            175: query_embedding(index_aligned_175, lemma),\n",
    "            176: query_embedding(index_aligned_176, lemma),\n",
    "        }\n",
    "    plot_trajectory(lemma_decade_embdding_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73d210a-6f57-4f0e-97eb-5aae67d32ee4",
   "metadata": {},
   "source": [
    "# aggregated analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b355bb-b4d3-4142-906b-3011708b471d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1901039-411b-45ad-8aa0-41eac1a0fd4f",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102bbf95-994c-4e05-8b39-be80e4fd8e9d",
   "metadata": {},
   "source": [
    "## plot_lemma_with_neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040caf0c-9fc7-4cc9-987d-3e3533b3049e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lemma_with_neighbours(decade_dict_value, lemma, height=None, width=None, rotation_degree=None):\n",
    "    neighbours_dict = query_related(decade_dict_value[1], lemma)\n",
    "    # print(neighbours_dict)\n",
    "    labels = [lemma]\n",
    "    values = [decade_dict_value[0][lemma][1]]\n",
    "    for neighbour in neighbours_dict.keys():\n",
    "        labels.append(neighbour)\n",
    "        values.append(decade_dict_value[0][neighbour][1])\n",
    "    plot_tsne_from_labels_embeddings(labels, values, None, height, width, rotation_degree)\n",
    "\n",
    "\n",
    "lemma = \"gehen\"\n",
    "plot_lemma_with_neighbours(decade_dict[174], lemma, height=500, width=500, rotation_degree=None)\n",
    "plot_lemma_with_neighbours(decade_dict[175], lemma, height=500, width=500, rotation_degree=180)\n",
    "plot_lemma_with_neighbours(decade_dict[176], lemma, height=500, width=500, rotation_degree=220)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061e78ce-a6bd-42fb-b496-1585096be8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_global_word_set(model_list):\n",
    "    word_set = set()\n",
    "    for m in model_list:\n",
    "        for w in m.wv.index_to_key:\n",
    "            word_set.add(w)\n",
    "    return word_set\n",
    "\n",
    "\n",
    "word_set = create_global_word_set(list(model_dict.values()))\n",
    "len(word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a564b082-fa77-4339-8c2f-80cda66e5d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_set_sample(word_set, model_list, limit, must_be_in_all_model):\n",
    "    word_set_sample = set()\n",
    "    num_found = 0\n",
    "    word_list_shuffled = list(word_set)\n",
    "    random.shuffle(word_list_shuffled)\n",
    "    for w in word_list_shuffled:\n",
    "        if num_found == limit:\n",
    "            break\n",
    "        else:\n",
    "            skip = False\n",
    "            if must_be_in_all_model:\n",
    "                for m in model_list:\n",
    "                    if w not in m.wv:\n",
    "                        skip = True\n",
    "                        break\n",
    "            if not skip:\n",
    "                word_set_sample.add(w)\n",
    "                num_found += 1\n",
    "    return word_set_sample\n",
    "\n",
    "\n",
    "word_set_sample = create_word_set_sample(word_set, list(model_dict.values()), 100, False)\n",
    "print(len(word_set_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089e395b-5458-48b3-a288-f5abe42713d2",
   "metadata": {},
   "source": [
    "## plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dba35c-8a3f-4258-ba8c-b27b2bea334a",
   "metadata": {},
   "source": [
    "## analysis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51740582-1840-4ca2-b231-82992614c131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184f18fa-b6c6-468b-834a-ed1ebc5a6f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gini_coefficient(x):\n",
    "    x = np.array(x, dtype=np.float64)\n",
    "    n = len(x)\n",
    "    x_sorted = np.sort(x)\n",
    "    cum_x = np.cumsum(x_sorted)\n",
    "    gini = (2 * np.sum((np.arange(1, n + 1) * x_sorted))) / (n * np.sum(x_sorted)) - (n + 1) / n\n",
    "    return gini\n",
    "\n",
    "\n",
    "calculate_gini_coefficient([1, 3, 2, 100, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f23f13-f929-4ef5-bac7-e66177abaa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diff_between_decades(w, model_prev, model_current, debug=False):\n",
    "    try:\n",
    "        v_prev = model_prev.wv[w]\n",
    "        v_current = model_current.wv[w]\n",
    "        word_set_top_10_prev = model_prev.wv.most_similar(w)\n",
    "        word_set_top_10_current = model_current.wv.most_similar(w)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    word_set_both = set()\n",
    "    for w_prev, _ in word_set_top_10_prev:\n",
    "        word_set_both.add(w_prev)\n",
    "    for w_current, _ in word_set_top_10_current:\n",
    "        word_set_both.add(w_current)\n",
    "\n",
    "    diff_per_decade = []\n",
    "    for w_rel in word_set_both:\n",
    "        try:\n",
    "            v_rel_prev = model_prev.wv[w_rel]\n",
    "            v_rel_current = model_current.wv[w_rel]\n",
    "        except:\n",
    "            pass\n",
    "        else:\n",
    "            cos_sim_prev = calculate_cosine_similarity(v_prev, v_rel_prev)\n",
    "            cos_sim_current = calculate_cosine_similarity(v_current, v_rel_current)\n",
    "            diff_prev_current = abs(cos_sim_prev - cos_sim_current)\n",
    "            diff_per_decade.append(diff_prev_current)\n",
    "            if debug:\n",
    "                print(\n",
    "                    \"w_rel:\",\n",
    "                    w_rel,\n",
    "                    \"cos_sim_prev:\",\n",
    "                    cos_sim_prev,\n",
    "                    \"cos_sim_current:\",\n",
    "                    cos_sim_current,\n",
    "                    \"diff_prev_current:\",\n",
    "                    diff_prev_current,\n",
    "                )\n",
    "    if diff_per_decade:\n",
    "        diff_per_decade = sum(diff_per_decade) / len(diff_per_decade)\n",
    "\n",
    "        return diff_per_decade\n",
    "\n",
    "\n",
    "get_diff_between_decades(\"gesetze\", model_dict[178], model_dict[179], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1e448b-4e30-4bb8-96c0-f1ca20b1cb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_relative_history(word_set, model_dict, print_progress=False):\n",
    "    len_total = len(word_set)\n",
    "    if print_progress:\n",
    "        len_segment = round(len_total / 100)\n",
    "        print(\"len_total:\", len_total)\n",
    "        print(\"len_segment:\", len_segment)\n",
    "    word_diff_history = {}\n",
    "    for i, w in enumerate(word_set):\n",
    "        if print_progress and i % len_segment == 0:\n",
    "            print(\"i:\", i)\n",
    "        model_prev = None\n",
    "        diff_between_decade_dict = {}\n",
    "        diff_between_decade_total = []\n",
    "        for decade, model_current in model_dict.items():\n",
    "            if model_prev:\n",
    "                diff_between_decade = get_diff_between_decades(w, model_prev, model_current)\n",
    "                if diff_between_decade:\n",
    "                    diff_between_decade_dict[str(decade) + \"0s\"] = diff_between_decade\n",
    "                    if 152 <= decade <= 191:\n",
    "                        diff_between_decade_total.append(diff_between_decade)\n",
    "            model_prev = model_current\n",
    "\n",
    "        if diff_between_decade_total:\n",
    "            diff_between_decade_avg = sum(diff_between_decade_total) / len(diff_between_decade_total)\n",
    "            diff_between_decade_gini = calculate_gini_coefficient(diff_between_decade_total)\n",
    "            word_diff_history[w] = {\n",
    "                \"diff_decade_avg\": diff_between_decade_avg,\n",
    "                \"diff_decade_gini\": diff_between_decade_gini,\n",
    "                \"diff_decade_dict\": diff_between_decade_dict,\n",
    "            }\n",
    "    return word_diff_history\n",
    "\n",
    "\n",
    "# word_diff_history = create_relative_history(word_set, model_dict, True)\n",
    "# word_diff_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7000b21e-99ac-4e8b-a223-c3d9c8869ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"/veld/output/word_diff_history.pkl\", \"wb\") as f:\n",
    "#    pickle.dump(word_diff_history, f)\n",
    "\n",
    "with open(\"/veld/output/word_diff_history.pkl\", \"rb\") as f:\n",
    "    word_diff_history = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5222b91-ecab-4825-aa50-33eecfaa2377",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_diff_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4590bb6e-a681-4370-aea2-182c05ee0e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_word_diff_history(word_diff_history, sort_key):\n",
    "    return dict(sorted(word_diff_history.items(), key=lambda item: -item[1][sort_key]))\n",
    "\n",
    "\n",
    "word_diff_history_sorted_avg = sort_word_diff_history(word_diff_history, \"diff_decade_avg\")\n",
    "word_diff_history_sorted_gini = sort_word_diff_history(word_diff_history, \"diff_decade_gini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f440a1-e142-4aed-8b36-ddd1bb1e9387",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, w_diff_history in enumerate(word_diff_history.items()):\n",
    "    if i == 3:\n",
    "        break\n",
    "    print(w_diff_history)\n",
    "\n",
    "print(\"------------\")\n",
    "\n",
    "for i, w_diff_history in enumerate(word_diff_history_sorted_avg.items()):\n",
    "    if i == 3:\n",
    "        break\n",
    "    print(w_diff_history)\n",
    "\n",
    "print(\"------------\")\n",
    "\n",
    "for i, w_diff_history in enumerate(word_diff_history_sorted_gini.items()):\n",
    "    if i == 3:\n",
    "        break\n",
    "    print(w_diff_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5595b688-9587-40bc-ba1e-4b8cb9a27450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(\n",
    "    word_diff_history,\n",
    "    from_top,\n",
    "    min_num_decades,\n",
    "    max_sample,\n",
    "    index_range=None,\n",
    "    word_set=None,\n",
    "):\n",
    "    word_diff_history_sample = {}\n",
    "    if from_top:\n",
    "        direction = 1\n",
    "    else:\n",
    "        direction = -1\n",
    "    if index_range:\n",
    "        if not index_range[0]:\n",
    "            index_range[0] = 0\n",
    "        if not index_range[1]:\n",
    "            index_range[1] = len(word_diff_history)\n",
    "    num_found = 0\n",
    "    for i, w_history in enumerate(list(word_diff_history.items())[::direction]):\n",
    "        w = w_history[0]\n",
    "        w_diff = w_history[1]\n",
    "        if ((word_set and w in word_set) or not word_set) and ((index_range and index_range[0] <= i < index_range[1]) or not index_range):\n",
    "            if num_found == max_sample:\n",
    "                break\n",
    "            if (min_num_decades and len(w_diff[\"diff_decade_dict\"]) >= min_num_decades) or not min_num_decades:\n",
    "                word_diff_history_sample[w] = w_diff\n",
    "                num_found += 1\n",
    "    return word_diff_history_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8a6c20-cd26-4e05-9afe-17d182e38e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sample(\n",
    "    word_diff_history_sorted_avg,\n",
    "    from_top=True,\n",
    "    min_num_decades=30,\n",
    "    max_sample=3,\n",
    "    index_range=[None, None],\n",
    "    word_set=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868b46b1-6796-4e8c-82e6-2ea092025a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(word_diff_history):\n",
    "    decades = []\n",
    "    for d in range(1470, 1960, 10):\n",
    "        decades.append(str(d) + \"s\")\n",
    "    data = {}\n",
    "    for w, diff_history in word_diff_history.items():\n",
    "        data[w] = diff_history[\"diff_decade_dict\"]\n",
    "    df = pd.DataFrame(data).reindex(decades).reset_index()\n",
    "    df = df.rename(columns={\"index\": \"Decade\"})\n",
    "    fig = go.Figure()\n",
    "    for category in data.keys():\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df[\"Decade\"],\n",
    "                y=df[category],\n",
    "                mode=\"lines+markers\",\n",
    "                name=category,\n",
    "                connectgaps=True,  # Ensures lines are drawn across missing data\n",
    "            )\n",
    "        )\n",
    "    fig.update_layout(\n",
    "        title=\"Data points by decade\",\n",
    "        xaxis_title=\"Decades\",\n",
    "        yaxis_title=\"Value\",\n",
    "        xaxis=dict(categoryorder=\"array\", categoryarray=decades),\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714da9c9-1673-4ad1-8315-b46cc7bb3c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot_tsne(vector_dict, title=None):\n",
    "\n",
    "    labels = []\n",
    "    values = []\n",
    "    for w in WORD_LIST:\n",
    "        labels.append(w)\n",
    "        values.append(vector_dict[w])\n",
    "    values = np.array(values)\n",
    "\n",
    "    tsne = TSNE(n_components=2, perplexity=5, random_state=42)\n",
    "    reduced_vectors_tsne = tsne.fit_transform(values)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(reduced_vectors_tsne[:, 0], reduced_vectors_tsne[:, 1], c=\"blue\", alpha=0.7)\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        plt.text(\n",
    "            reduced_vectors_tsne[i, 0],\n",
    "            reduced_vectors_tsne[i, 1],\n",
    "            label,\n",
    "            fontsize=9,\n",
    "            ha=\"right\",\n",
    "            color=\"black\",\n",
    "        )\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c858e1-4898-4a7b-b5fa-4e03b966180c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(\n",
    "    get_sample(\n",
    "        word_diff_history_sorted_avg,\n",
    "        from_top=True,\n",
    "        min_num_decades=30,\n",
    "        max_sample=3,\n",
    "        index_range=[9000, None],\n",
    "        word_set=None,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22893874-abfe-4ac3-a805-9c8544ce0ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(\n",
    "    get_sample(\n",
    "        word_diff_history_sorted_gini,\n",
    "        from_top=True,\n",
    "        min_num_decades=30,\n",
    "        max_sample=3,\n",
    "        index_range=[10000, None],\n",
    "        word_set=None,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7339f55-73c1-4dc6-9dc3-3c62d0b19644",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\n",
    "    get_cos_sim_sample(\n",
    "        word_relative_history_list,\n",
    "        from_top=True,\n",
    "        min_num_decades=30,\n",
    "        max_sample=3,\n",
    "        index_range=[1200, None],\n",
    "        word_set=None,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac6c72b-9bd5-458b-be99-2ef426b30f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\n",
    "    get_cos_sim_sample(\n",
    "        word_relative_history_list,\n",
    "        from_top=True,\n",
    "        min_num_decades=30,\n",
    "        num_sample=3,\n",
    "        index_range=[2000, None],\n",
    "        word_set=None,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef058173-696e-4d88-930a-102a89d9ab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\n",
    "    get_cos_sim_sample(\n",
    "        word_relative_history_list,\n",
    "        from_top=True,\n",
    "        min_num_decades=30,\n",
    "        num_sample=3,\n",
    "        index_range=[10000, None],\n",
    "        word_set=None,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e510dd-f95f-4988-ac3e-63d7ca82d715",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\n",
    "    get_cos_sim_sample(\n",
    "        word_relative_history_list,\n",
    "        from_top=False,\n",
    "        min_num_decades=30,\n",
    "        num_sample=3,\n",
    "        index_range=[None, None],\n",
    "        word_set=None,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514ebc36-282d-4330-be40-4986fb639d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\n",
    "    get_cos_sim_sample(\n",
    "        word_relative_history_list,\n",
    "        from_top=False,\n",
    "        min_num_decades=10,\n",
    "        num_sample=3,\n",
    "        index_range=[40000, None],\n",
    "        word_set=None,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c098a8-7302-41e9-b609-8a3bc141309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in word_relative_history_list:\n",
    "    if w[0] in [\"dar\", \"sollst\"]:\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90d735b-6ce0-46cf-8e35-ce2d2a032f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\n",
    "    get_cos_sim_sample(\n",
    "        word_relative_history_list,\n",
    "        from_top=True,\n",
    "        min_num_decades=30,\n",
    "        num_sample=3,\n",
    "        index_range=[None, None],\n",
    "        word_set=[\"gesetz\", \"himmel\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6a8853-238d-4eaf-b0fa-51c6477736c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\n",
    "    get_cos_sim_sample(\n",
    "        word_relative_history_list,\n",
    "        from_top=True,\n",
    "        min_num_decades=None,\n",
    "        num_sample=3,\n",
    "        index_range=[None, None],\n",
    "        word_set=[\"demokratie\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32565c9-d9c8-4133-be55-8dab3345bcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\n",
    "    get_cos_sim_sample(\n",
    "        word_relative_history_list,\n",
    "        from_top=True,\n",
    "        min_num_decades=None,\n",
    "        num_sample=3,\n",
    "        index_range=[None, None],\n",
    "        word_set=[\"frau\", \"mann\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34983f1a-c03a-4f0a-a829-4eeecab676de",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\n",
    "    get_cos_sim_sample(\n",
    "        word_relative_history_list,\n",
    "        from_top=True,\n",
    "        min_num_decades=None,\n",
    "        num_sample=3,\n",
    "        index_range=[None, None],\n",
    "        word_set=[\"mutter\", \"vater\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf94c1c6-ef35-4ae3-8ea6-bf92928aa931",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\n",
    "    get_cos_sim_sample(\n",
    "        word_relative_history_list,\n",
    "        from_top=True,\n",
    "        min_num_decades=None,\n",
    "        num_sample=3,\n",
    "        index_range=[None, None],\n",
    "        word_set=[\"könig\", \"kaiser\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1da5f9-e988-47e5-874b-98660d99211d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\n",
    "    get_cos_sim_sample(\n",
    "        word_relative_history_list,\n",
    "        from_top=True,\n",
    "        min_num_decades=None,\n",
    "        num_sample=4,\n",
    "        index_range=[None, None],\n",
    "        word_set=[\"wasser\", \"erde\", \"brot\", \"haus\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0848ce1e-cf04-4516-9aaa-3329e23cb31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\n",
    "    get_cos_sim_sample(\n",
    "        word_relative_history_list,\n",
    "        from_top=True,\n",
    "        min_num_decades=None,\n",
    "        num_sample=10,\n",
    "        index_range=[None, None],\n",
    "        word_set=[\"haus\", \"kaiser\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897aa4af-abbe-421b-aeb6-a765f5f05486",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\n",
    "    get_cos_sim_sample(\n",
    "        word_relative_history_list,\n",
    "        from_top=True,\n",
    "        min_num_decades=None,\n",
    "        num_sample=10,\n",
    "        index_range=[None, None],\n",
    "        word_set=[\"mensch\", \"gott\", \"welt\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448e3365-6124-4459-8db2-c65658f56026",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\n",
    "    get_cos_sim_sample(\n",
    "        word_relative_history_list_gini,\n",
    "        from_top=True,\n",
    "        min_num_decades=20,\n",
    "        num_sample=5,\n",
    "        index_range=[None, None],\n",
    "        word_set=None,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6733b0-65d4-40d6-8fc2-5e5c94c5c67f",
   "metadata": {},
   "source": [
    "## various snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d2e9d2-7d00-4c5c-b4eb-af255e408422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hnswlib\n",
    "import numpy as np\n",
    "\n",
    "# Your data\n",
    "texts = [\"apple\", \"banana\", \"orange\"]\n",
    "vectors = np.random.rand(3, 128).astype(\"float32\")\n",
    "vectors /= np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "\n",
    "# Map text labels to integer IDs\n",
    "text_to_id = {text: i for i, text in enumerate(texts)}\n",
    "id_to_text = {i: text for text, i in text_to_id.items()}\n",
    "\n",
    "# Initialize the index\n",
    "dim = 128\n",
    "index = hnswlib.Index(space=\"cosine\", dim=dim)\n",
    "index.init_index(max_elements=10, ef_construction=100, M=16)\n",
    "index.add_items(vectors, ids=list(text_to_id.values()))\n",
    "\n",
    "# Query\n",
    "query_vector = vectors[0].reshape(1, -1)\n",
    "labels, distances = index.knn_query(query_vector, k=2)\n",
    "\n",
    "# Convert back to text labels\n",
    "results = [(id_to_text[label], dist) for label, dist in zip(labels[0], distances[0])]\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c306d5e3-0127-4789-8dd2-89718aaa6269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: probably not useful\n",
    "def create_word_cos_sim_history_list(word_set, model_dict):\n",
    "    word_cos_sim_history_list = []\n",
    "    for w in word_set:\n",
    "        cos_sim_history_dict = {}\n",
    "        model_prev = None\n",
    "        w_vec_current = None\n",
    "        w_vec_prev = None\n",
    "        total_diff = 0\n",
    "        for decade, model_current in model_dict.items():\n",
    "            try:\n",
    "                w_vec_current = model_current.wv[w]\n",
    "            except:\n",
    "                pass\n",
    "            else:\n",
    "                if w_vec_prev is not None:\n",
    "                    cos_sim = calculate_cosine_similarity(w_vec_prev, w_vec_current)\n",
    "                    total_diff += 2 - (cos_sim + 1)\n",
    "                    cos_sim_history_dict[str(decade) + \"0s\"] = cos_sim\n",
    "                w_vec_prev = w_vec_current\n",
    "        if cos_sim_history_dict:\n",
    "            word_cos_sim_history_list.append((w, total_diff, cos_sim_history_dict))\n",
    "    word_cos_sim_history_list = sorted(word_cos_sim_history_list, key=lambda x: -x[1])\n",
    "    return word_cos_sim_history_list\n",
    "\n",
    "\n",
    "# word_cos_sim_history_list = create_word_cos_sim_history_list(word_set, model_dict)\n",
    "# print(len(word_cos_sim_history_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2932a747-0393-4950-8efd-9e99cbe111ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Define the full range of decades\n",
    "decades = [f\"{1900+10*i}s\" for i in range(10)]  # 1900s to 2000s\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    \"a\": {\"1920s\": 0.6, \"1930s\": 0.5, \"1980s\": 0.1},\n",
    "    \"b\": {\"1930s\": 0.2, \"1980s\": 0.4},\n",
    "}\n",
    "\n",
    "# Convert to DataFrame with explicit ordering\n",
    "df = pd.DataFrame(data).reindex(decades).reset_index()\n",
    "df = df.rename(columns={\"index\": \"Decade\"})\n",
    "\n",
    "# Create the plot\n",
    "fig = go.Figure()\n",
    "\n",
    "for category in data.keys():\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df[\"Decade\"],\n",
    "            y=df[category],\n",
    "            mode=\"lines+markers\",\n",
    "            name=category,\n",
    "            connectgaps=True,  # Ensures lines are drawn across missing data\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Format the layout\n",
    "fig.update_layout(\n",
    "    title=\"Data points by decade\",\n",
    "    xaxis_title=\"Decades\",\n",
    "    yaxis_title=\"Value\",\n",
    "    xaxis=dict(categoryorder=\"array\", categoryarray=decades),\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fef6ebb-a05c-418b-83e4-6f0cd2483d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    \"a\": {\"1920s\": 1, \"1930s\": 0.5, \"1980s\": 0.1},\n",
    "    \"b\": {\"1930s\": 0.2, \"1940s\": 0, \"1950s\": 0.7, \"1980s\": 0.4},\n",
    "}\n",
    "\n",
    "# Define all decades (ensuring consistent x-axis)\n",
    "decades = [f\"{1900 + 10*i}s\" for i in range(10)]\n",
    "\n",
    "# Plot each ID\n",
    "plt.figure(figsize=(10, 5))\n",
    "for identifier, values in data.items():\n",
    "    y_values = [values.get(decade, None) for decade in decades]  # Use None for missing data\n",
    "    plt.plot(decades, y_values, marker=\"o\", label=identifier)\n",
    "\n",
    "# Customize plot\n",
    "plt.xlabel(\"Decades\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Data points by decade\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1ac837-d39b-4ffd-8df6-39521490dc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    \"a\": {\"1920s\": 1, \"1930s\": 0.5, \"1980s\": 0.1},\n",
    "    \"b\": {\"1930s\": 0.2, \"1950s\": 0.8, \"1980s\": 0.4},\n",
    "}\n",
    "\n",
    "# Define all decades (ensuring consistent x-axis)\n",
    "decades = [f\"{1900 + 10*i}s\" for i in range(10)]\n",
    "\n",
    "# Plot each ID\n",
    "plt.figure(figsize=(10, 5))\n",
    "for identifier, values in data.items():\n",
    "    y_values = [values.get(decade, np.nan) for decade in decades]  # Use np.nan to avoid connecting missing points\n",
    "    plt.plot(decades, y_values, marker=\"o\", label=identifier)\n",
    "\n",
    "# Customize plot\n",
    "plt.xlabel(\"Decades\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Data points by decade\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891147e3-7c00-48b7-9a73-791aded94d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    \"a\": {\"1920s\": 1, \"1930s\": 0.5, \"1980s\": 0.1},\n",
    "    \"b\": {\"1930s\": 0.2, \"1950s\": 0.8, \"1980s\": 0.4},\n",
    "}\n",
    "\n",
    "# Define all decades (ensuring consistent x-axis)\n",
    "decades = [f\"{1900 + 10*i}s\" for i in range(10)]\n",
    "\n",
    "# Convert to a DataFrame for interpolation\n",
    "df = pd.DataFrame({key: {d: data[key].get(d, np.nan) for d in decades} for key in data})\n",
    "\n",
    "# Interpolate missing values (linear interpolation)\n",
    "df.interpolate(method=\"linear\", inplace=True)\n",
    "\n",
    "# Plot each ID\n",
    "plt.figure(figsize=(10, 5))\n",
    "for identifier in df.columns:\n",
    "    plt.plot(decades, df[identifier], marker=\"o\", label=identifier)\n",
    "\n",
    "# Customize plot\n",
    "plt.xlabel(\"Decades\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Data points by decade (with interpolation)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0417801-7682-4af3-ba54-e5df6fdbde61",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3346ed0-7322-4b92-a238-ae0ddbab5814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    \"Decade\": [\"1920s\", \"1930s\", \"1980s\", \"1930s\", \"1950s\", \"1980s\"],\n",
    "    \"Value\": [1, 0.5, 0.1, 0.2, 0.8, None],\n",
    "    \"Category\": [\"a\", \"a\", \"a\", \"b\", \"b\", \"b\"],\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Plot using Plotly Express\n",
    "fig = px.line(\n",
    "    df,\n",
    "    x=\"Decade\",\n",
    "    y=\"Value\",\n",
    "    color=\"Category\",\n",
    "    markers=True,\n",
    "    title=\"Data points by decade\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fface3c-db05-4dce-a67d-989fa4d5555d",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_cosine_similarity(model_dict[180].wv[\"mann\"], model_dict[180].wv[\"mann\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8159f0e-3a99-4ef3-9bfa-ea579c339de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict[180].wv.most_similar(\"mann\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106baaa6-bc50-4779-b630-081e491fee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict[181].wv.most_similar(\"mann\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f86932a-4c32-4538-a978-0f5e1288513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract vocabulary and corresponding vectors\n",
    "words = list(model.wv.index_to_key)[:100]  # List of words in vocabulary\n",
    "vectors = np.array([model.wv[word] for word in words])  # Word vectors\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "similarity_matrix = cosine_similarity(vectors)\n",
    "\n",
    "# Convert to a dictionary of word pairs (optional)\n",
    "similarity_dict = {(words[i], words[j]): similarity_matrix[i, j] for i in range(len(words)) for j in range(len(words)) if i != j}\n",
    "\n",
    "# Example: Print top 10 most similar word pairs\n",
    "sorted_pairs = sorted(similarity_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "for pair, sim in sorted_pairs[:10]:\n",
    "    print(f\"{pair}: {sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9531e0-fa23-42c4-b3af-e45a5f8fd7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = cosine_similarity(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964175a4-799e-4622-821d-fe82e01db546",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d402296b-fcfa-4349-8b16-4759ba466aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a dictionary of word pairs (optional)\n",
    "similarity_dict = {(words[i], words[j]): similarity_matrix[i, j] for i in range(len(words)) for j in range(len(words)) if i != j}\n",
    "\n",
    "# Example: Print top 10 most similar word pairs\n",
    "sorted_pairs = sorted(similarity_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "for pair, sim in sorted_pairs:\n",
    "    print(f\"{pair}: {sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9e36a4-c2b2-49e3-ae3b-bdc36a4401dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
